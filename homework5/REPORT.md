# Predicting Lorenz System Behavior with Feed Forward, Long Short Term Memory, Recurrent, and Echo State Networks

**Author**:

Ara Vardanyan

**Abstract**:

In this study, we investigate the application of various neural network architectures, including feed-forward neural networks, Long Short-Term Memory (LSTM), Recurrent Neural Networks (RNN), and Echo State Networks (ESN), to predict future states of the Lorenz system.


---

## Introduction

This report aims to explore the application and performance of various neural network architectures in predicting future states of the Lorenz system, a set of three differential equations known for their chaotic behavior.

In the study, we train feed-forward neural networks, Long Short-Term Memory (LSTM), Recurrent Neural Networks (RNN), and Echo State Networks (ESN) to advance the solution from time t to t + ∆t for specific values of the Lorenz system parameter ρ (ρ = 10, 28, and 40). We then evaluate the trained networks' ability to predict future states for different values of ρ (ρ = 17 and ρ = 35). This task allows us to assess the networks' capacity to generalize from the training data and accurately forecast the dynamics of the Lorenz system under different conditions.

Through training each of these networks on the Lorenz system data and comparing their performances in terms of their ability to predict future states, we aim to gain insights into the strengths and weaknesses of each neural network architecture in handling complex, non-linear time series data, such as those generated by the Lorenz system. This understanding will inform the selection and development of neural network architectures for similar tasks in the future.

---

## Theoretical Background

### The Lorenz System:
The Lorenz System is a set of three differential equations that were originally derived by Edward Lorenz in 1963 as a simplified model of atmospheric convection. The system is known for its chaotic behavior and its sensitivity to initial conditions, which is often referred to as the "butterfly effect". The equations are:

dx/dt = σ(y - x)

dy/dt = x(ρ - z) - y

dz/dt = xy - βz

where x, y, and z make up the system state, t is time, and σ, ρ, β are system parameters.

### Neural Networks (NN):
Neural Networks are a machine learning model inspired by the human brain. They consist of layers of nodes or "neurons" that can learn to make predictions or decisions by learning from data. Neural Networks are especially effective at processing complex, high-dimensional data, such as images or text, and can learn to capture non-linear relationships between inputs and outputs.

### Feedforward Neural Networks (FNNs):
Feedforward Neural Networks are a type of NN where the connections between nodes do not form a cycle. In FNNs, the information moves in only one direction: from the input layer, through the hidden layers, to the output layer. This is the main difference from recurrent neural networks, which have connections forming a cycle.

### Long Short-Term Memory (LSTM):
LSTM is a type of recurrent neural network (RNN) that can learn and remember patterns over long sequences. Unlike traditional RNNs, which suffer from the vanishing or exploding gradient problem, LSTMs use gates and a cell state to control the flow of information, making them more effective at capturing long-term dependencies.

### Recurrent Neural Networks (RNNs):
RNNs are a type of NN designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or the time series data. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs, making them ideal for such tasks.

### Echo State Networks (ESN):
ESNs are a type of recurrent neural network with a sparsely connected hidden layer (referred to as the reservoir). The weights of the input-hidden connections and hidden-hidden connections are randomly initialized and then fixed. Only the weights of the connections from the hidden layer to the output layer are trained. This leads to a simpler, faster training process.

In this assignment, we will use these different types of neural networks to predict the future states of the Lorenz system for different values of the parameter ρ. The performance of these networks will be evaluated based on how accurately they can forecast the dynamics of the system.

---

## Algorithm Implementation and Development

Importing necessary libraries:

```
import numpy as np
from scipy import integrate
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import SimpleRNN
import tensorflow_addons as tfa
import tensorflow as tf
```

Generating Lorenz system data:
```
def lorenz_deriv(x_y_z, t0, sigma=10, beta=8/3, rho=28):
    x, y, z = x_y_z
    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]


def generate_data(rho, seed=123):
    dt = 0.01
    T = 8
    t = np.arange(0, T + dt, dt)

    np.random.seed(seed)
    x0 = -15 + 30 * np.random.random((100, 3))

    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(10, 8/3, rho)) for x0_j in x0])

    nn_input = np.zeros((100 * (len(t) - 1), 3))
    nn_output = np.zeros_like(nn_input)

    for j in range(100):
        nn_input[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, :-1, :]
        nn_output[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, 1:, :]

    return nn_input, nn_output
```

### Functions for creating and training models

#### Feed Forward Neural Network (FFNN)
```
def build_ffnn(input_shape):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=input_shape))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(input_shape[0], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_ffnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Long Short Term Memory Network (LSTM)
```
def build_lstm(input_shape):
    model = Sequential()
    model.add(LSTM(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_lstm(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Recurrent Neural Network (RNN)
```
def build_rnn(input_shape):
    model = Sequential()
    model.add(SimpleRNN(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_rnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Echo State Network (ESN)
```
def create_esn(input_shape, units, connectivity=0.1, leaky=1, spectral_radius=0.9):
    inputs = tf.keras.Input(shape=input_shape)
    esn_outputs = tfa.layers.ESN(units, connectivity, leaky, spectral_radius)(inputs)
    output = tf.keras.layers.Dense(3)(esn_outputs)
    
    model = tf.keras.Model(inputs=inputs, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')
    return model

def train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs=50, batch_size=32):
    esn_model = create_esn(input_shape, reservoir_size)
    esn_history = esn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)
    return esn_model, esn_history
```

### Training models

Preparing data to train models:
```
# Generate data for each rho value
inputs_10, targets_10 = generate_data(10)
inputs_28, targets_28 = generate_data(28)
inputs_40, targets_40 = generate_data(40)

# Concatenate the inputs and targets
inputs = np.concatenate([inputs_10, inputs_28, inputs_40])
targets = np.concatenate([targets_10, targets_28, targets_40])
```

#### Feed Forward Neural Network (FFNN)
```
# Build the model
ffnn = build_ffnn(inputs.shape[1:])

# Train the model
train_ffnn(ffnn, inputs, targets, epochs=25)
```

Reshape inputs for LSTM, RNN, and ESN [samples, time steps, features]
```
inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1]))
```

#### Long Short Term Memory Network (LSTM)
```
# Build the model
lstm = build_lstm(inputs.shape[1:])

# Train the model
train_lstm(lstm, inputs, targets, epochs=25)
```

#### Recurrent Neural Network (RNN)
```
# Build the model
rnn = build_rnn(inputs.shape[1:])

# Train the model
train_rnn(rnn, inputs, targets, epochs=25)
```

#### Echo State Network (ESN)
```
# Split data into training and test sets
train_size = int(0.8 * len(inputs))
X_train, X_test = inputs[:train_size], inputs[train_size:]
y_train, y_test = targets[:train_size], targets[train_size:]

# Define ESN parameters
input_shape = inputs.shape[1:]
reservoir_size = 64
epochs = 25
batch_size = 32

# Train the ESN model
esn_model, esn_history = train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs, batch_size)
```

### Predicting Lorenz System for rho = 17, 35

Defining function for plotting predictions:
```
def plot_predictions(new_targets, predictions, title):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(new_targets[:, 0], new_targets[:, 1], new_targets[:, 2], 'r', label='Actual')
    ax.plot(predictions[:, 0], predictions[:, 1], predictions[:, 2], 'b', label='Predicted')
    ax.set_title(title)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.legend()
    plt.show()
```

#### Predicting with FFNN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Use the trained model to make predictions for the new data
predictions_17 = ffnn.predict(inputs_17)

mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'FFNN Mean Squared Error on data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'FFNN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Use the trained model to make predictions for the new data
predictions_35 = ffnn.predict(inputs_35)

mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'FFNN Mean Squared Error on data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'FFNN for rho={rho_35}')
```

### Predicting with LSTM
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = lstm.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'LSTM Mean Squared Error on data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'LSTM for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = lstm.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'LSTM Mean Squared Error on data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'LSTM for rho={rho_35}')
```

### Predicting with RNN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = rnn.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'RNN Mean Squared Error on new data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'RNN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = rnn.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'RNN Mean Squared Error on new data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'RNN for rho={rho_35}')
```

### Predicting with ESN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = esn_model.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'ESN Mean Squared Error on new data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'ESN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = esn_model.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'ESN Mean Squared Error on new data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'ESN for rho={rho_35}')
```

---

## Computational Results

### Error on Lorenz system with unseen values of rho

Epochs: 25

For rho=17:

| Model | Mean Squared Error |
|-------|-------------------|
| RNN   | 0.006688 |
| LSTM  | 0.007858 |
| FFNN  | 0.009169 |
| ESN   | 7.348020 |

For rho=35:

| Model | Mean Squared Error |
|-------|-------------------|
| LSTM  | 0.012208 |
| RNN   | 0.014535 |
| FFNN  | 0.018447 |
| ESN   | 9.366474 |

### Figures of Predicted vs Actual for FFNN, LSTM, RNN, and ESN for rho=17, 35

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/ffnn_predictions_rho_17.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/ffnn_predictions_rho_35.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/lstm_predictions_rho_17.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/lstm_predictions_rho_35.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/rnn_predictions_rho_17.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/rnn_predictions_rho_35.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/esn_predictions_rho_17.png'>
</p>

<p>
  <img src='https://github.com/ara-vardanyan/EE-399-Machine-Learning-HW-Reports/blob/686628772fe8867e813148ff1d39f0c95545d441/homework5/figures/esn_predictions_rho_35.png'>
</p>


---

## Summary and Conclusions

In this report, we first investigated the performance of various neural network architectures, including Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Feed-Forward Neural Networks (FFNN), and Echo State Networks (ESN), in predicting future states of the Lorenz system for unseen values of the parameter ρ.

We trained each of these networks to advance the solution from time t to t + ∆t for specific values of ρ (ρ = 10, 28, and 40) and then evaluated their performance in predicting future states for ρ = 17 and ρ = 35. The performance of the networks was assessed based on the mean squared error (MSE) of their predictions.

For ρ = 17, the RNN model achieved the lowest MSE (0.006688), followed by the LSTM model (0.007858), the FFNN model (0.009169), and the ESN model (7.348020). For ρ = 35, the LSTM model achieved the lowest MSE (0.012208), followed by the RNN model (0.014535), the FFNN model (0.018447), and the ESN model (9.366474).

From these results, we can conclude that the RNN and LSTM models outperformed the FFNN and ESN models in predicting future states of the Lorenz system for unseen values of ρ. The ESN model, in particular, performed significantly worse than the other models, suggesting that it may not be well-suited to this task.

These findings provide valuable insights into the strengths and weaknesses of different neural network architectures in handling complex, non-linear time series data, such as those generated by the Lorenz system. They highlight the importance of selecting the appropriate architecture for the task at hand and provide a basis for further investigation into the factors that contribute to the superior performance of RNN and LSTM models in this context.